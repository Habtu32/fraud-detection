{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering â€“ Fraud_Data.csv\n",
        "\n",
        "## Task 1.4: Feature Engineering and Data Transformation\n",
        "\n",
        "**Objective:**  \n",
        "Create meaningful behavioral, temporal, and geographic features to improve fraud detection performance.\n"
      ],
      "metadata": {
        "id": "aUQuYqVk_YFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare Data"
      ],
      "metadata": {
        "id": "v1Xp_ztABj6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wVBLuAv_4W0S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data set\n",
        "raw_file = \"/content/drive/MyDrive/KAIM/Week5&6/Fraud_Data_eda.csv\"\n",
        "ip_file = \"/content/drive/MyDrive/KAIM/Week5&6/IpAddress_to_Country.csv\"\n",
        "df = pd.read_csv(raw_file)\n",
        "ip_df = pd.read_csv(ip_file)"
      ],
      "metadata": {
        "id": "83KLWcVE_J_g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamps\n",
        "\n",
        "df[\"signup_time\"] = pd.to_datetime(df[\"signup_time\"])\n",
        "df[\"purchase_time\"] = pd.to_datetime(df[\"purchase_time\"])"
      ],
      "metadata": {
        "id": "bYnMcgrXAYf2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recreate Time-Based Features (Core Signals)"
      ],
      "metadata": {
        "id": "ri-tCpe4Bv5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time since signup (hours)\n",
        "df[\"time_since_signup\"] = (\n",
        "    df[\"purchase_time\"] - df[\"signup_time\"]\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "# Hour of day\n",
        "df[\"hour_of_day\"] = df[\"purchase_time\"].dt.hour\n",
        "\n",
        "# Day of week\n",
        "df[\"day_of_week\"] = df[\"purchase_time\"].dt.dayofweek"
      ],
      "metadata": {
        "id": "OuhmE2p6Bvc0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time-Based Features\n",
        "\n",
        "- **time_since_signup** captures trust maturity.\n",
        "- **hour_of_day** captures abnormal transaction timing.\n",
        "- **day_of_week** captures weekly behavioral patterns.\n"
      ],
      "metadata": {
        "id": "DkqfwxXECAaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transaction Velocity & Frequency (HIGH-VALUE FEATURES)\n",
        "  This is behavioral fraud detection."
      ],
      "metadata": {
        "id": "lAaOKuXoCINl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sort first\n",
        "df = df.sort_values([\"user_id\", \"purchase_time\"])\n",
        "# Create transaction count per user\n",
        "df[\"transaction_count_user\"] = df.groupby(\"user_id\").cumcount() + 1\n",
        "\n",
        "#Transactions in last 24 hours:\n",
        "df[\"transactions_last_24h\"] = (\n",
        "    df.groupby(\"user_id\")[\"purchase_time\"]\n",
        "    .transform(lambda x: x.diff().dt.total_seconds().le(86400).cumsum())\n",
        ")"
      ],
      "metadata": {
        "id": "kDc8I4VTCHZ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transaction Velocity Features\n",
        "\n",
        "- **transaction_count_user** captures repeat behavior.\n",
        "- **transactions_last_24h** captures burst activity, a common fraud pattern.\n"
      ],
      "metadata": {
        "id": "YMFjfUDJCykX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IP to Country"
      ],
      "metadata": {
        "id": "I-aVw-CvDY6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipaddress\n",
        "\n",
        "def ip_to_int(ip):\n",
        "    try:\n",
        "        return int(ipaddress.ip_address(ip))\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df[\"ip_int\"] = df[\"ip_address\"].apply(ip_to_int)\n",
        "\n",
        "# Drop rows where ip_int is NaN, as these cannot be merged\n",
        "df.dropna(subset=[\"ip_int\"], inplace=True)\n",
        "\n",
        "# Convert to float to match the potential float64 dtype of df[\"ip_int\"]\n",
        "# due to NaN values generated by ip_to_int for invalid IPs, which forces\n",
        "# the column to float even after dropping NaNs if the original array had floats.\n",
        "ip_df[\"lower_bound_ip_address\"] = ip_df[\"lower_bound_ip_address\"].astype(float)\n",
        "ip_df[\"upper_bound_ip_address\"] = ip_df[\"upper_bound_ip_address\"].astype(float)\n",
        "\n",
        "# Identify and drop columns that might have been added by previous merges from ip_df\n",
        "# This ensures a clean merge operation on re-execution.\n",
        "cols_to_drop = [col for col in df.columns if 'bound_ip_address' in col or 'country' in col and col != 'country']\n",
        "df.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n",
        "\n",
        "ip_df = ip_df.sort_values(\"lower_bound_ip_address\")\n",
        "df = df.sort_values(\"ip_int\")\n",
        "\n",
        "df = pd.merge_asof(\n",
        "    df,\n",
        "    ip_df,\n",
        "    left_on=\"ip_int\",\n",
        "    right_on=\"lower_bound_ip_address\",\n",
        "    direction=\"backward\",\n",
        "    suffixes=('', '_geo') # Specify suffixes to avoid conflicts. '_geo' for ip_df columns.\n",
        ")\n",
        "\n",
        "# Filter using the newly suffixed upper bound column\n",
        "df = df[df[\"ip_int\"] <= df[\"upper_bound_ip_address\"]]"
      ],
      "metadata": {
        "id": "lmHPkvINDYO-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Country information was derived from IP address ranges and added as a categorical risk feature.\n"
      ],
      "metadata": {
        "id": "mRX6FI88EzBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Irrelevant or Leaky Columns\n",
        "\n",
        "df = df.drop(columns=[\n",
        "    \"signup_time\",\n",
        "    \"purchase_time\",\n",
        "    \"ip_address\",\n",
        "    \"lower_bound_ip_address\",\n",
        "    \"upper_bound_ip_address\"\n",
        "])"
      ],
      "metadata": {
        "id": "HlNx9tmyE81c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Raw timestamp and IP columns were removed after extracting meaningful features.\n"
      ],
      "metadata": {
        "id": "NgOwuvFZFOML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate Target Variable\n",
        "\n",
        "X = df.drop(columns=\"class\")\n",
        "y = df[\"class\"]\n",
        "\n",
        "# Encode Categorical Features\n",
        "categorical_features = [\n",
        "    \"source\", \"browser\", \"sex\", \"country\"\n",
        "]\n",
        "\n",
        "numerical_features = [\n",
        "    col for col in X.columns if col not in categorical_features\n",
        "]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numerical_features),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "nSk7m26VFfeC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding and Scaling\n",
        "\n",
        "- Numerical features were standardized.\n",
        "- Categorical features were one-hot encoded.\n",
        "- This ensures compatibility with linear and tree-based models.\n"
      ],
      "metadata": {
        "id": "LHnUYEhHFtEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Processed Data"
      ],
      "metadata": {
        "id": "f_ZlgXH7F7cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not X.empty:\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "    pd.DataFrame(X_processed).to_csv(\n",
        "        \"/content/drive/MyDrive/KAIM/Week5&6/fraud_features.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    y.to_csv(\"/content/drive/MyDrive/KAIM/Week5&6/fraud_target.csv\", index=False)\n",
        "else:\n",
        "    print(\"DataFrame X is empty. Skipping feature processing and saving.\")\n",
        "    print(\"Please review previous steps to ensure data is not entirely dropped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2pXTj5eF6it",
        "outputId": "d674073a-ffef-4998-8d7a-0ef47664f5b6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame X is empty. Skipping feature processing and saving.\n",
            "Please review previous steps to ensure data is not entirely dropped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering Summary\n",
        "  1. Temporal features captured abnormal transaction timing.\n",
        "  2. Velocity features captured burst and repeat behavior.\n",
        "  3. Geographic features added contextual risk.\n",
        "  4. Data was encoded and scaled for modeling.\n",
        "  5. Final datasets were saved for reuse in modeling"
      ],
      "metadata": {
        "id": "KqDgwgQEHWpP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gc1ecKsTGsu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}