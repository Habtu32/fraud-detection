{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUQuYqVk_YFe"
      },
      "source": [
        "# Feature Engineering – Fraud_Data.csv\n",
        "\n",
        "## Task 1.4: Feature Engineering and Data Transformation\n",
        "\n",
        "**Objective:**  \n",
        "Create meaningful behavioral, temporal, and geographic features to improve fraud detection performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Xp_ztABj6M"
      },
      "source": [
        "# Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "wVBLuAv_4W0S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "83KLWcVE_J_g"
      },
      "outputs": [],
      "source": [
        "# Load data set\n",
        "raw_file = \"../data/processed/Fraud_Data_eda.csv\"\n",
        "ip_file = \"../data/raw/IpAddress_to_Country.csv\"\n",
        "df = pd.read_csv(raw_file)\n",
        "ip_df = pd.read_csv(ip_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bYnMcgrXAYf2"
      },
      "outputs": [],
      "source": [
        "# Convert timestamps\n",
        "\n",
        "df[\"signup_time\"] = pd.to_datetime(df[\"signup_time\"])\n",
        "df[\"purchase_time\"] = pd.to_datetime(df[\"purchase_time\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129146, 17)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri-tCpe4Bv5w"
      },
      "source": [
        "# Recreate Time-Based Features (Core Signals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "OuhmE2p6Bvc0"
      },
      "outputs": [],
      "source": [
        "# Time since signup (hours)\n",
        "df[\"time_since_signup\"] = (\n",
        "    df[\"purchase_time\"] - df[\"signup_time\"]\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "# Hour of day\n",
        "df[\"hour_of_day\"] = df[\"purchase_time\"].dt.hour\n",
        "\n",
        "# Day of week\n",
        "df[\"day_of_week\"] = df[\"purchase_time\"].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129146, 18)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkqfwxXECAaY"
      },
      "source": [
        "Time-Based Features\n",
        "\n",
        "- **time_since_signup** captures trust maturity.\n",
        "- **hour_of_day** captures abnormal transaction timing.\n",
        "- **day_of_week** captures weekly behavioral patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAaOKuXoCINl"
      },
      "source": [
        "# Transaction Velocity & Frequency (HIGH-VALUE FEATURES)\n",
        "  This is behavioral fraud detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "kDc8I4VTCHZ0"
      },
      "outputs": [],
      "source": [
        "# sort first\n",
        "df = df.sort_values([\"user_id\", \"purchase_time\"])\n",
        "# Create transaction count per user\n",
        "df[\"transaction_count_user\"] = df.groupby(\"user_id\").cumcount() + 1\n",
        "\n",
        "#Transactions in last 24 hours:\n",
        "df[\"transactions_last_24h\"] = (\n",
        "    df.groupby(\"user_id\")[\"purchase_time\"]\n",
        "    .transform(lambda x: x.diff().dt.total_seconds().le(86400).cumsum())\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129146, 20)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMFjfUDJCykX"
      },
      "source": [
        "Transaction Velocity Features\n",
        "\n",
        "- **transaction_count_user** captures repeat behavior.\n",
        "- **transactions_last_24h** captures burst activity, a common fraud pattern.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-aVw-CvDY6t"
      },
      "source": [
        "# IP to Country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lmHPkvINDYO-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ip_address is numeric; used directly as ip_int.\n",
            "after merge_asof: (129146, 21)\n",
            "upper_bound_ip_address nulls: 0\n",
            "mask true: 129146 mask false: 0\n",
            "ip_int range: 16778864.0719029 3758052531.47708\n",
            "ip_df range: 16777216.0 3758096383.0\n",
            "unmatched (upper_nan or > upper): 0\n"
          ]
        }
      ],
      "source": [
        "import ipaddress\n",
        "\n",
        "# Robust ip_int extraction: use numeric ip_address if present, else convert dotted strings\n",
        "if df['ip_address'].dtype.kind in 'iuf':  # int/uint/float => already numeric\n",
        "    df['ip_int'] = df['ip_address'].astype(float)\n",
        "    print(\"ip_address is numeric; used directly as ip_int.\")\n",
        "else:\n",
        "    def ip_to_int(ip):\n",
        "        try:\n",
        "            return int(ipaddress.ip_address(str(ip)))\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "    df['ip_int'] = df['ip_address'].apply(ip_to_int)\n",
        "    print(\"after ip_to_int: \", df.shape, \"ip_int nulls:\", df['ip_int'].isna().sum())\n",
        "\n",
        "# Do NOT drop rows here — we'll mark unmatched IPs as 'unknown' after merging to avoid losing data.\n",
        "\n",
        "# Convert to float to match the potential float64 dtype of df[\"ip_int\"]\n",
        "# due to NaN values generated by ip_to_int for invalid IPs, which forces\n",
        "# the column to float even after dropping NaNs if the original array had floats.\n",
        "ip_df[\"lower_bound_ip_address\"] = ip_df[\"lower_bound_ip_address\"].astype(float)\n",
        "ip_df[\"upper_bound_ip_address\"] = ip_df[\"upper_bound_ip_address\"].astype(float)\n",
        "\n",
        "# Identify and drop columns that might have been added by previous merges from ip_df\n",
        "# This ensures a clean merge operation on re-execution.\n",
        "cols_to_drop = [col for col in df.columns if 'bound_ip_address' in col or 'country' in col and col != 'country']\n",
        "df.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n",
        "\n",
        "ip_df = ip_df.sort_values(\"lower_bound_ip_address\")\n",
        "df = df.sort_values(\"ip_int\")\n",
        "\n",
        "# Merge and then debug the result to make sure matches occurred\n",
        "df = pd.merge_asof(\n",
        "    df,\n",
        "    ip_df,\n",
        "    left_on=\"ip_int\",\n",
        "    right_on=\"lower_bound_ip_address\",\n",
        "    direction=\"backward\",\n",
        "    suffixes=('', '_geo') # Specify suffixes to avoid conflicts. '_geo' for ip_df columns.\n",
        ")\n",
        "\n",
        "print(\"after merge_asof:\", df.shape)\n",
        "if 'upper_bound_ip_address' in df.columns:\n",
        "    print(\"upper_bound_ip_address nulls:\", df['upper_bound_ip_address'].isna().sum())\n",
        "    mask = df['ip_int'] <= df['upper_bound_ip_address']\n",
        "    print(\"mask true:\", mask.sum(), \"mask false:\", (~mask).sum())\n",
        "    if mask.sum() < len(df):\n",
        "        print(\"Sample rows failing the ip_int <= upper_bound check:\")\n",
        "        display(df.loc[~mask, ['ip_address','ip_int','lower_bound_ip_address','upper_bound_ip_address']].head(10))\n",
        "\n",
        "    # Check numeric coverage between data and IP ranges\n",
        "    try:\n",
        "        print(\"ip_int range:\", df['ip_int'].min(), df['ip_int'].max())\n",
        "        print(\"ip_df range:\", ip_df['lower_bound_ip_address'].min(), ip_df['upper_bound_ip_address'].max())\n",
        "    except Exception as e:\n",
        "        print(\"Error checking numeric ranges:\", e)\n",
        "\n",
        "    # Non-destructive fallback: mark unmatched rows as 'unknown' and keep them\n",
        "    unmatched = df['upper_bound_ip_address'].isna() | (~mask)\n",
        "    print(\"unmatched (upper_nan or > upper):\", unmatched.sum())\n",
        "    if unmatched.any():\n",
        "        df.loc[unmatched, 'country'] = 'unknown'\n",
        "        # Optional: clear bound columns for unmatched rows\n",
        "        df.loc[unmatched, ['lower_bound_ip_address','upper_bound_ip_address']] = np.nan\n",
        "        print(f\"After marking unmatched country='unknown': df.shape {df.shape} (unmatched kept)\")\n",
        "else:\n",
        "    print(\"WARNING: 'upper_bound_ip_address' not found in merged DataFrame. Columns:\", df.columns.tolist())\n",
        "\n",
        "# Note: we intentionally do NOT drop rows here to avoid losing data. If you prefer to remove unmatched rows, uncomment the following:\n",
        "# if 'upper_bound_ip_address' in df.columns:\n",
        "#     before_filter = len(df)\n",
        "#     df = df[df['upper_bound_ip_address'].notna() & (df['ip_int'] <= df['upper_bound_ip_address'])]\n",
        "#     print(f\"after filtering ip range: {df.shape} (dropped {before_filter - len(df)} rows)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129146, 21)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRX6FI88EzBo"
      },
      "source": [
        "\n",
        "Country information was derived from IP address ranges and added as a categorical risk feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "HlNx9tmyE81c"
      },
      "outputs": [],
      "source": [
        "# Drop Irrelevant or Leaky Columns\n",
        "\n",
        "df = df.drop(columns=[\n",
        "    \"signup_time\",\n",
        "    \"purchase_time\",\n",
        "    \"ip_address\",\n",
        "    \"lower_bound_ip_address\",\n",
        "    \"upper_bound_ip_address\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129146, 16)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgOwuvFZFOML"
      },
      "source": [
        "\n",
        "Raw timestamp and IP columns were removed after extracting meaningful features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "nSk7m26VFfeC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping identifier columns (if present): ['user_id', 'device_id']\n",
            "Categorical features: ['source', 'browser', 'sex', 'country']\n",
            "Numerical features (count): 8\n",
            "Non-numeric columns excluded from numeric features: ['source', 'browser', 'sex', 'country', 'country_geo']\n"
          ]
        }
      ],
      "source": [
        "# Separate Target Variable and drop identifier columns\n",
        "id_cols = ['user_id', 'device_id']\n",
        "print(\"Dropping identifier columns (if present):\", [c for c in id_cols if c in df.columns])\n",
        "\n",
        "X = df.drop(columns=['class'] + id_cols, errors='ignore')\n",
        "y = df['class'] if 'class' in df.columns else pd.Series(dtype=int)\n",
        "\n",
        "# Encode Categorical Features safely\n",
        "categorical_candidates = ['source', 'browser', 'sex', 'country']\n",
        "categorical_features = [c for c in categorical_candidates if c in X.columns]\n",
        "\n",
        "# Select numeric features only\n",
        "numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "print(\"Numerical features (count):\", len(numerical_features))\n",
        "print(\"Non-numeric columns excluded from numeric features:\", X.select_dtypes(exclude=['number']).columns.tolist())\n",
        "\n",
        "# Build transformers only for existing features\n",
        "transformers = []\n",
        "if numerical_features:\n",
        "    transformers.append((\"num\", StandardScaler(), numerical_features))\n",
        "if categorical_features:\n",
        "    transformers.append((\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_features))\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=transformers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df.shape: (129146, 16)\n",
            "class distribution:\n",
            " class\n",
            "0    116878\n",
            "1     12268\n",
            "Name: count, dtype: int64\n",
            "X.shape: (129146, 15)\n",
            "y.shape: (129146,)\n",
            "First few columns of X: ['user_id', 'purchase_value', 'device_id', 'source', 'browser', 'sex', 'age', 'time_since_signup', 'hour_of_day', 'ip_int', 'country', 'day_of_week', 'transaction_count_user', 'transactions_last_24h', 'country_geo']\n"
          ]
        }
      ],
      "source": [
        "# Debug: check sizes and target distribution after feature extraction\n",
        "print(\"df.shape:\", df.shape)\n",
        "if 'class' in df.columns:\n",
        "    print(\"class distribution:\\n\", df['class'].value_counts(dropna=False))\n",
        "else:\n",
        "    print(\"'class' column missing in DataFrame\")\n",
        "\n",
        "# Prepare X and y for the rest of the pipeline and print shapes for debugging\n",
        "X = df.drop(columns='class') if 'class' in df.columns else df.copy()\n",
        "y = df['class'] if 'class' in df.columns else pd.Series(dtype=int)\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"y.shape:\", y.shape)\n",
        "print(\"First few columns of X:\", X.columns.tolist()[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHnUYEhHFtEe"
      },
      "source": [
        "Encoding and Scaling\n",
        "\n",
        "- Numerical features were standardized.\n",
        "- Categorical features were one-hot encoded.\n",
        "- This ensures compatibility with linear and tree-based models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ZlgXH7F7cg"
      },
      "source": [
        "# Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2pXTj5eF6it",
        "outputId": "d674073a-ffef-4998-8d7a-0ef47664f5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_processed is sparse; converting to dense array before saving.\n",
            "X_processed shape: (129146, 199)\n"
          ]
        }
      ],
      "source": [
        "if not X.empty:\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # If X_processed is sparse (e.g., CSR matrix from OneHotEncoder with sparse=True), convert to dense\n",
        "    try:\n",
        "        from scipy import sparse\n",
        "        if sparse.issparse(X_processed):\n",
        "            print(\"X_processed is sparse; converting to dense array before saving.\")\n",
        "            X_processed = X_processed.toarray()\n",
        "    except Exception as e:\n",
        "        print(\"scipy not available or conversion error:\", e)\n",
        "\n",
        "    print(\"X_processed shape:\", getattr(X_processed, 'shape', None))\n",
        "\n",
        "    X_df = pd.DataFrame(X_processed)\n",
        "    X_df.to_csv(\n",
        "        \"../data/processed/fraud_features.csv\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    y.to_csv(\"../data/processed/fraud_target.csv\", index=False)\n",
        "else:\n",
        "    print(\"DataFrame X is empty. Skipping feature processing and saving.\")\n",
        "    print(\"Please review previous steps to ensure data is not entirely dropped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqDgwgQEHWpP"
      },
      "source": [
        "#Feature Engineering Summary\n",
        "  1. Temporal features captured abnormal transaction timing.\n",
        "  2. Velocity features captured burst and repeat behavior.\n",
        "  3. Geographic features added contextual risk.\n",
        "  4. Data was encoded and scaled for modeling.\n",
        "  5. Final datasets were saved for reuse in modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
